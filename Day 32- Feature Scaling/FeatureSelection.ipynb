{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why do Feature Selection ??\n",
    "\n",
    "# Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "# Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "# Reduces Training Time: Less data means that algorithms train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.0\n",
      "0.23.4\n",
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import sklearn\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print (sklearn.__version__)\n",
    "print (pd.__version__)\n",
    "print (np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class takes care for scaling the features to the scale of 0-1\n",
    "# we are doing the scaling with this cap because we use sigmoid activation fxn in logistic which \n",
    "# also has the range from 0-1\n",
    "class Normalizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sc = MinMaxScaler()\n",
    "    \n",
    "    def scale(self, X, dtype):\n",
    "        if dtype=='train':\n",
    "            XX = self.sc.fit_transform(X)\n",
    "        elif dtype=='test':\n",
    "            XX = self.sc.transform(X)\n",
    "        else:\n",
    "            return None\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METHOD-1\n",
    "# Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  \\\n",
      "87           88           6.3          2.3           4.4          1.3   \n",
      "111         112           6.4          2.7           5.3          1.9   \n",
      "\n",
      "        Species  \n",
      "87   versicolor  \n",
      "111   virginica  \n",
      "********************\n",
      "[[0.58389262 0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.74496644 0.58333333 0.29166667 0.72881356 0.75      ]] ['versicolor' 'virginica']\n",
      "********************\n",
      "[22.522 11.377  4.471 26.862 30.478]\n",
      "********************\n",
      "[[0.576 0.5  ]\n",
      " [0.729 0.75 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select those features that have strong relationship with the output variable\n",
    "df = pd.read_csv('H:/SELF/Yashu/Files/iris.csv')\n",
    "df = shuffle(df)\n",
    "print (df.head(2))\n",
    "print ('*'*20)\n",
    "norm = Normalizer()\n",
    "X = norm.scale(df.iloc[:,:-1].values, 'train')\n",
    "Y = df.iloc[:,-1].values\n",
    "print (X[:2], Y[:2])\n",
    "print ('*'*20)\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=2)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "print ('*'*20)\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METHOD-2\n",
    "# Recursive Feature Elimination (Backwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width  \\\n",
      "87           88           6.3          2.3           4.4          1.3   \n",
      "100         101           6.3          3.3           6.0          2.5   \n",
      "\n",
      "        Species  \n",
      "87   versicolor  \n",
      "100   virginica  \n",
      "********************\n",
      "[[0.584 0.556 0.125 0.576 0.5  ]\n",
      " [0.671 0.556 0.542 0.847 1.   ]] ['versicolor' 'virginica']\n",
      "********************\n",
      "Selected Features: [ True False False False  True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# The Recursive Feature Elimination (or RFE) works by recursively removing attributes and\n",
    "# building a model on those attributes that remain. It uses the model accuracy to identify which attributes \n",
    "# (and combination of attributes) contribute the most to predicting the target attribute\n",
    "\n",
    "df = pd.read_csv('H:/SELF/Yashu/Files/iris.csv')\n",
    "df = shuffle(df)\n",
    "print (df.head(2))\n",
    "print ('*'*20)\n",
    "norm = Normalizer()\n",
    "X = norm.scale(df.iloc[:,:-1].values, 'train')\n",
    "Y = df.iloc[:,-1].values\n",
    "print (X[:2], Y[:2])\n",
    "print ('*'*20)\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 2)\n",
    "fit = rfe.fit(X, Y)\n",
    "print((\"Selected Features: %s\") % fit.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### METHOD-3\n",
    "# Using Feature importance from any bagging algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
      "36          37           5.5          3.5           1.3          0.2  setosa\n",
      "43          44           5.0          3.5           1.6          0.6  setosa\n",
      "********************\n",
      "[[0.242 0.333 0.625 0.051 0.042]\n",
      " [0.289 0.194 0.625 0.102 0.208]] ['setosa' 'setosa']\n",
      "********************\n",
      "[0.326 0.032 0.034 0.2   0.408]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('H:/SELF/Yashu/Files/iris.csv')\n",
    "df = shuffle(df)\n",
    "print (df.head(2))\n",
    "print ('*'*20)\n",
    "norm = Normalizer()\n",
    "X = norm.scale(df.iloc[:,:-1].values, 'train')\n",
    "Y = df.iloc[:,-1].values\n",
    "print (X[:2], Y[:2])\n",
    "print ('*'*20)\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print (model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Elimination are mutually exclusive techniques for getting correct set of features for\n",
    "### your ML model\n",
    "\n",
    "## Few techniques for elimination are:\n",
    "### Remove Zero/Less Variance columns\n",
    "### Remove Columns with many missing values\n",
    "### Remove Highly +ve/-ve co-related features\n",
    "\n",
    "## Other techniques for feature selection include: \n",
    "### PCA Decomposition\n",
    "### Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
