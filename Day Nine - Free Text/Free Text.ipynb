{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\"This Document focuses on processing free text data\",\n",
    "          \"This also includes natural language processing\",\n",
    "          \"text processing via bag of words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document', 'This', 'also', 'bag', 'data', 'focuses', 'free', 'includes', 'language', 'natural', 'of', 'on', 'processing', 'text', 'via', 'words']\n"
     ]
    }
   ],
   "source": [
    "document_words=[doc.split() for doc in documents]\n",
    "vocab=sorted(set(sum(document_words,[])))\n",
    "vocab_dict={k:i for i,k in enumerate(vocab)}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Document': 0, 'This': 1, 'also': 2, 'bag': 3, 'data': 4, 'focuses': 5, 'free': 6, 'includes': 7, 'language': 8, 'natural': 9, 'of': 10, 'on': 11, 'processing': 12, 'text': 13, 'via': 14, 'words': 15}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0]\n",
      " [0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Creating  a matrix that contains word counts (term frequencies) for all the documents\n",
    "X=np.zeros((len(documents),len(vocab)), dtype=int)\n",
    "for i,doc in enumerate (document_words):\n",
    "    for word in doc:\n",
    "        X[i,vocab_dict[word]]+=1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.09861229 0.40546511 1.09861229 1.09861229 1.09861229 1.09861229\n",
      " 1.09861229 1.09861229 1.09861229 1.09861229 1.09861229 1.09861229\n",
      " 0.         0.40546511 1.09861229 1.09861229]\n"
     ]
    }
   ],
   "source": [
    "# Compute inverse document frequency for our data set as follows, which mainly just requires counting how many documents contain each word\n",
    "idf=np.log(X.shape[0]/X.astype(bool).sum(axis=0))\n",
    "print (idf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.09861229 0.40546511 0.         0.         1.09861229 1.09861229\n",
      "  1.09861229 0.         0.         0.         0.         1.09861229\n",
      "  0.         0.40546511 0.         0.        ]\n",
      " [0.         0.40546511 1.09861229 0.         0.         0.\n",
      "  0.         1.09861229 1.09861229 1.09861229 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.09861229 0.         0.\n",
      "  0.         0.         0.         0.         1.09861229 0.\n",
      "  0.         0.40546511 1.09861229 1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "# Scales the columns of the term frequency matrix by their inverse document frequency\n",
    "Xidf=X*idf\n",
    "print(Xidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.02916832 0.02916832]\n",
      " [0.02916832 1.         0.        ]\n",
      " [0.02916832 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# The cosine similarity is a number between zero (meaning the two documents share no terms in common) and one (meaning the two documents have the exact same term frequency or TFIDF representation)\n",
    "Xnorm=Xidf/np.linalg.norm(Xidf,axis=1)[:,None]\n",
    "M=Xnorm@Xnorm.T\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"pittsburgh has some excellent new restaurants\",\n",
    "    \"boston is a city with great cuisine\",\n",
    "    \"postgresql is a relational database management system\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'boston': 1, 'city': 2, 'cuisine': 3, 'database': 4, 'excellent': 5, 'great': 6, 'has': 7, 'is': 8, 'management': 9, 'new': 10, 'pittsburgh': 11, 'postgresql': 12, 'relational': 13, 'restaurants': 14, 'some': 15, 'system': 16, 'with': 17} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_words = [doc.split() for doc in documents]\n",
    "vocab = sorted(set(sum(document_words, [])))\n",
    "vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "print(vocab_dict, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "epit=np.zeros(len(vocab))\n",
    "epit[vocab_dict[\"pittsburgh\"]]=1\n",
    "print(epit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16073254 0.43550663 0.43550663 0.43550663 0.43550663 0.16073254\n",
      "  0.43550663 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.16073254 0.         0.         0.         0.         0.16073254\n",
      "  0.         0.43550663 0.43550663 0.43550663 0.43550663 0.43550663]]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gs.corpora.Dictionary(document_words)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in document_words]\n",
    "tfidf = gs.models.TfidfModel(corpus)\n",
    "X_tfidf = gs.matutils.corpus2csc(tfidf[corpus])\n",
    "print(X_tfidf.todense().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        1.        0.0516699]\n",
      " [0.        0.0516699 1.       ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "M = gs.similarities.MatrixSimilarity(tfidf[corpus])\n",
    "print(M.get_similarities(tfidf[corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
